package com.upgrad.kafkaspark;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.connect.json.JsonDeserializer;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFlatMapFunction;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

import scala.Tuple2;

public final class KafkaSparkDemo {

	public static void main(String[] args) {
		try {
			String brokers = "52.55.237.11:9092";
			String groupId = "ud-gro-abi";
			String topics = "stockData";
			SparkConf sparkConf = new SparkConf().setAppName("KafkaSparkDemo").setMaster("local[*]");
			JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.minutes(1));
			jssc.sparkContext().setLogLevel("WARN");
			Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(",")));
			// Define a new HashMap for holding the kafka information.
			Map<String, Object> kafkaParams = new HashMap<>();
			kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);
			kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
			kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
			kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
			JavaInputDStream<ConsumerRecord<String, JsonNode>> messages = KafkaUtils.createDirectStream(jssc,
					LocationStrategies.PreferConsistent(), ConsumerStrategies.Subscribe(topicsSet, kafkaParams));
			JavaDStream<StockData> lines = messages.map(new Function<ConsumerRecord<String, JsonNode>, StockData>() {
				private static final long serialVersionUID = 1L;

				// overriding the default call method
				@Override
				public StockData call(ConsumerRecord<String, JsonNode> record) throws JsonProcessingException {
					System.out.println("inside call");
					System.out.println(record.value());
					ObjectMapper objectMapper = new ObjectMapper();

					return objectMapper.treeToValue(record.value(), StockData.class);
				}

			});

			lines.print();

			JavaPairDStream<String, Double> currencyAndClosePrice = lines
					.flatMapToPair(new PairFlatMapFunction<StockData, String, Double>() {

						private static final long serialVersionUID = 1L;

						@Override
						public Iterator<Tuple2<String, Double>> call(StockData t) throws Exception {
							// TODO Auto-generated method stub
							
							List<Tuple2<String, Double>> list = new ArrayList<Tuple2<String, Double>>();
							String symbol = t.getSymbol();
							double doubleClosingPrice = t.getPriceData().getClose();
							list.add(new Tuple2<String, Double>(symbol, doubleClosingPrice));

							return list.iterator();
						}
					});
			
			currencyAndClosePrice.print();
			
			/*
			 * JavaPairDStream<String, Double> windowedWordCounts =
			 * currencyAndClosePrice.reduceByKeyAndWindow((i1, i2) -> i1.getCount(),
			 * Durations.minutes(5), Durations.minutes(10));
			 */
			// Start the streaming computation
			jssc.start();
			
			jssc.awaitTermination();

		} catch (Exception e) {
			e.printStackTrace();

		}
	}
}

package com.upgrad.kafkaspark;

import java.io.Serializable;

public class PriceData implements Serializable{


	/**
	 * 
	 */
	private double close;
	private double open;
	private double high;
	private double low;
	private double volume;
	public double getClose() {
		return close;
	}
	public void setClose(double close) {
		this.close = close;
	}
	public double getOpen() {
		return open;
	}
	public void setOpen(double open) {
		this.open = open;
	}
	public double getHigh() {
		return high;
	}
	public void setHigh(double high) {
		this.high = high;
	}
	public double getLow() {
		return low;
	}
	public void setLow(double low) {
		this.low = low;
	}
	public double getVolume() {
		return volume;
	}
	public void setVolume(double volume) {
		this.volume = volume;
	}
	
}

package com.upgrad.kafkaspark;

import java.io.Serializable;

public class StockData implements Serializable{

	private String symbol;
	private String timestamp;
	private PriceData priceData;

	public String getSymbol() {
		return symbol;
	}

	public void setSymbol(String symbol) {
		this.symbol = symbol;
	}

	public String getTimestamp() {
		return timestamp;
	}

	public void setTimestamp(String timestamp) {
		this.timestamp = timestamp;
	}

	public PriceData getPriceData() {
		return priceData;
	}

	public void setPriceData(PriceData priceData) {
		this.priceData = priceData;
	}

	@Override
	public String toString() {
		return "StockData [symbol=" + symbol + ", timestamp=" + timestamp + ", priceData=" + priceData + "]";
	}
	
	

}

<project xmlns="http://maven.apache.org/POM/4.0.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<groupId>com.demo.kafkaspark</groupId>
	<artifactId>KafkaSparkDemo</artifactId>
	<version>0.0.3-SNAPSHOT</version>

	<dependencies>
		<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10_2.11 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
			<version>2.2.0</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10 -->
		<!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.module/jackson-module-scala -->
		<dependency>
			<groupId>com.fasterxml.jackson.module</groupId>
			<artifactId>jackson-module-scala_2.11</artifactId>
			<version>2.10.2</version>
		</dependency>

		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming_2.11</artifactId>
			<version>2.2.0</version>
			<scope>provided</scope>
		</dependency>

		<!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
		<dependency>
			<groupId>org.apache.kafka</groupId>
			<artifactId>kafka-clients</artifactId>
			<version>2.0.0</version>
		</dependency>
		<dependency>
			<groupId>org.apache.kafka</groupId>
			<artifactId>connect-json</artifactId>
			<version>1.0.1</version>
		</dependency>
		<!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core -->
		<!-- https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-core -->
		<dependency>
			<groupId>com.fasterxml.jackson.core</groupId>
			<artifactId>jackson-core</artifactId>
			<version>2.10.3</version>
		</dependency>


	</dependencies>
	<build>
		<plugins>
			<plugin>
				<artifactId>maven-assembly-plugin</artifactId>
				<configuration>
					<descriptorRefs>
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
				</configuration>
				<executions>
					<execution>
						<id>make-my-jar-with-dependencies</id>
						<phase>package</phase>
						<goals>
							<goal>single</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<version>3.1.0</version>
				<executions>
					<execution>
						<phase>package</phase>
						<goals>
							<goal>shade</goal>
						</goals>
						<configuration>
							<artifactSet>
								<includes>
									<include>org.apache.spark:spark-streaming-kafka-0-10_2.11</include>
									<include>org.apache.kafka:kafka-clients</include>
								</includes>
							</artifactSet>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>
	</build>
	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
	</properties>


</project>
